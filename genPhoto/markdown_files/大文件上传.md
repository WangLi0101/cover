### 大文件上传

#### 客户端

客户端需要完成的事有两件

1. 将文件切片
使用 slice 方法

```ts
export const createChunk = (
  file: File,
  chunkSize = 10 * 1024 * 1024
): Blob[] => {
  const chunks = [];
  for (let i = 0; i < file.size; i += chunkSize) {
    const chunck = file.slice(i, i + chunkSize);
    chunks.push(chunck);
  }
  return chunks;
};
```

2. 生成文件的hash值
由于文件会很大，将整个文件读取到内存中生成hash值可能会卡死。因此使用分片生成。由于生成可能会导致效率问题，推荐使用webWorker来生成

- 生成hash使用 spark-md5 库

webworker文件

```ts
import SparkMD5 from "spark-md5";
const generateHash = (chunks: Blob[]) => {
  return new Promise(resolve => {
    const spark = new SparkMD5.ArrayBuffer();
    for (let i = 0; i < chunks.length; i++) {
      const reader = new FileReader();
      reader.readAsArrayBuffer(chunks[i]);
      reader.onload = function (e) {
        spark.append(e.target?.result as ArrayBuffer);
        if (i === chunks.length - 1) {
          resolve(spark.end());
        }
      };
    }
  });
};

self.onmessage = async (event: MessageEvent) => {
  const chunks = event.data.chunks;
  const hash = await generateHash(chunks);
  self.postMessage({
    hash: hash
  });
};

```

```ts
const worker = new Worker(
  new URL("@/webworker/fileworker.ts", import.meta.url),
  {
    type: "module"
  }
);
export const generateHash = (chunks: Blob[]): Promise<string> => {
  return new Promise(resolve => {
    worker.postMessage({
      chunks: chunks
    });
    worker.onmessage = function (e) {
      resolve(e.data.hash);
    };
  });
};
```

#### 服务端

服务端的主要任务是将各个分片进行合并

```ts
  async concatFile(data: CompileDto) {
    const { fileName, hash, total } = data;
    try {
      const uploadDir = path.join(process.cwd(), 'uploads', hash, 'tmp');
      const finalFileName = fileName || hash;
      const finalFilePath = path.join(
        process.cwd(),
        'uploads',
        hash,
        finalFileName,
      );

      // 确保目标目录存在
      const targetDir = path.dirname(finalFilePath);
      if (!fs.existsSync(targetDir)) {
        fs.mkdirSync(targetDir, { recursive: true });
      }

      return new Promise((resolve, reject) => {
        // 创建可写流
        const writeStream = fs.createWriteStream(finalFilePath);

        // 按顺序处理每个分片
        const mergeChunks = async (chunkIndex: number) => {
          if (chunkIndex >= total) {
            // 所有分片已处理完毕，关闭写入流
            writeStream.end();
            return;
          }

          const chunkPath = path.join(uploadDir, `${hash}-${chunkIndex}`);

          if (!fs.existsSync(chunkPath)) {
            reject(new Error(`找不到分片文件: ${chunkPath}`));
            writeStream.end();
            return;
          }

          // 读取分片文件
          const chunkBuffer = fs.readFileSync(chunkPath);

          // 写入到目标文件
          writeStream.write(chunkBuffer, (err) => {
            if (err) {
              reject(err);
              return;
            }

            // 删除已处理的分片
            fs.unlinkSync(chunkPath);

            // 处理下一个分片
            mergeChunks(chunkIndex + 1);
          });
        };

        // 开始合并第一个分片
        mergeChunks(0);

        // 处理写入完成事件
        writeStream.on('finish', () => {
          console.log('文件合并完成:', finalFilePath);
          resolve({ success: true, path: finalFilePath });
        });

        // 处理错误
        writeStream.on('error', (err) => {
          console.error('文件合并错误:', err);
          reject({
            success: false,
            message: '文件合并失败',
            error: err.message,
          });
        });
      });
    } catch (error) {
      console.error('文件合并失败:', error);
      return { success: false, message: '文件合并失败', error: error.message };
    }
  }
```
